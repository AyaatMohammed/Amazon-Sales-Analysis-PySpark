{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T14:36:46.433764Z",
     "iopub.status.busy": "2025-09-02T14:36:46.433321Z",
     "iopub.status.idle": "2025-09-02T14:36:54.964068Z",
     "shell.execute_reply": "2025-09-02T14:36:54.962751Z",
     "shell.execute_reply.started": "2025-09-02T14:36:46.433717Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OlistECommerceAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"PySpark installed and SparkSession created successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T14:38:47.357362Z",
     "iopub.status.busy": "2025-09-02T14:38:47.356929Z",
     "iopub.status.idle": "2025-09-02T14:38:59.618650Z",
     "shell.execute_reply": "2025-09-02T14:38:59.617292Z",
     "shell.execute_reply.started": "2025-09-02T14:38:47.357335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_path = \"/kaggle/input/brazilian-ecommerce/\"\n",
    "\n",
    "orders_df = spark.read.csv(input_path + \"olist_orders_dataset.csv\", header=True, inferSchema=True)\n",
    "order_items_df = spark.read.csv(input_path + \"olist_order_items_dataset.csv\", header=True, inferSchema=True)\n",
    "customers_df = spark.read.csv(input_path + \"olist_customers_dataset.csv\", header=True, inferSchema=True)\n",
    "products_df = spark.read.csv(input_path + \"olist_products_dataset.csv\", header=True, inferSchema=True)\n",
    "order_payments_df = spark.read.csv(input_path + \"olist_order_payments_dataset.csv\", header=True, inferSchema=True)\n",
    "sellers_df = spark.read.csv(input_path + \"olist_sellers_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Successfully loaded 6 data tables!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T14:39:45.989461Z",
     "iopub.status.busy": "2025-09-02T14:39:45.989113Z",
     "iopub.status.idle": "2025-09-02T14:39:46.429364Z",
     "shell.execute_reply": "2025-09-02T14:39:46.428047Z",
     "shell.execute_reply.started": "2025-09-02T14:39:45.989438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"--- Orders Table Schema ---\")\n",
    "orders_df.printSchema()\n",
    "\n",
    "print(\"\\n--- First 5 rows of Orders Data ---\")\n",
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T14:41:48.391396Z",
     "iopub.status.busy": "2025-09-02T14:41:48.390179Z",
     "iopub.status.idle": "2025-09-02T14:41:48.692563Z",
     "shell.execute_reply": "2025-09-02T14:41:48.691082Z",
     "shell.execute_reply.started": "2025-09-02T14:41:48.391357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"--- Order Items Table Schema ---\")\n",
    "order_items_df.printSchema()\n",
    "\n",
    "print(\"\\n--- First 5 rows of Order Items Data ---\")\n",
    "order_items_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T14:42:19.657568Z",
     "iopub.status.busy": "2025-09-02T14:42:19.657215Z",
     "iopub.status.idle": "2025-09-02T14:42:25.299712Z",
     "shell.execute_reply": "2025-09-02T14:42:25.296466Z",
     "shell.execute_reply.started": "2025-09-02T14:42:19.657543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merged_df = orders_df.join(order_items_df, on=\"order_id\", how=\"inner\")\n",
    "print(f\"Number of rows in the merged table: {merged_df.count()}\")\n",
    "print(\"\\n--- Merged Table Schema ---\")\n",
    "merged_df.printSchema()\n",
    "\n",
    "print(\"\\n--- First 5 rows of the Merged Table ---\")\n",
    "merged_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T14:43:27.988596Z",
     "iopub.status.busy": "2025-09-02T14:43:27.988181Z",
     "iopub.status.idle": "2025-09-02T14:43:31.201829Z",
     "shell.execute_reply": "2025-09-02T14:43:31.200738Z",
     "shell.execute_reply.started": "2025-09-02T14:43:27.988569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "delivered_df = merged_df.filter(col(\"order_status\") == \"delivered\")\n",
    "print(f\"Number of delivered orders: {delivered_df.count()}\")\n",
    "print(\"\\n--- Checking for Null Values ---\")\n",
    "delivered_df.select([col(c).isNull().cast(\"integer\").alias(c) for c in [\"order_purchase_timestamp\", \"price\", \"freight_value\"]]).groupBy().sum().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T14:44:00.853905Z",
     "iopub.status.busy": "2025-09-02T14:44:00.853605Z",
     "iopub.status.idle": "2025-09-02T14:44:02.384965Z",
     "shell.execute_reply": "2025-09-02T14:44:02.383636Z",
     "shell.execute_reply.started": "2025-09-02T14:44:00.853886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "top_10_expensive_items = delivered_df.orderBy(col(\"price\").desc()).limit(10)\n",
    "print(\"--- Top 10 Most Expensive Products Sold ---\")\n",
    "top_10_expensive_items.select(\"order_id\", \"product_id\", \"price\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T14:44:53.062443Z",
     "iopub.status.busy": "2025-09-02T14:44:53.062054Z",
     "iopub.status.idle": "2025-09-02T14:44:55.646177Z",
     "shell.execute_reply": "2025-09-02T14:44:55.643679Z",
     "shell.execute_reply.started": "2025-09-02T14:44:53.062417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, col, desc\n",
    "\n",
    "full_df = delivered_df.join(customers_df, on=\"customer_id\", how=\"inner\")\n",
    "revenue_by_city = full_df.groupBy(\"customer_city\", \"customer_state\") \\\n",
    ".agg(sum(\"price\").alias(\"total_revenue\"))\n",
    "top_10_cities_by_revenue = revenue_by_city.orderBy(col(\"total_revenue\").desc()).limit(10)\n",
    "print(\"--- Top 10 Cities by Total Revenue ---\")\n",
    "top_10_cities_by_revenue.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T14:45:26.409311Z",
     "iopub.status.busy": "2025-09-02T14:45:26.408883Z",
     "iopub.status.idle": "2025-09-02T14:45:28.814341Z",
     "shell.execute_reply": "2025-09-02T14:45:28.812376Z",
     "shell.execute_reply.started": "2025-09-02T14:45:26.409287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, sum, col, desc\n",
    "sales_by_month_df = full_df.withColumn(\"year\", year(col(\"order_purchase_timestamp\"))) \\\n",
    "                           .withColumn(\"month\", month(col(\"order_purchase_timestamp\")))\n",
    "revenue_by_month = sales_by_month_df.groupBy(\"year\", \"month\") \\\n",
    "                                    .agg(sum(\"price\").alias(\"total_revenue\"))\n",
    "top_months_by_revenue = revenue_by_month.orderBy(col(\"total_revenue\").desc())\n",
    "print(\"--- Top Months by Total Revenue (Across All Years) ---\")\n",
    "top_months_by_revenue.show(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T14:48:52.295154Z",
     "iopub.status.busy": "2025-09-02T14:48:52.294831Z",
     "iopub.status.idle": "2025-09-02T14:48:55.925308Z",
     "shell.execute_reply": "2025-09-02T14:48:55.924464Z",
     "shell.execute_reply.started": "2025-09-02T14:48:52.295136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_with_revenue = full_df.withColumn(\"revenue_after_freight\", col(\"price\") - col(\"freight_value\"))\n",
    "\n",
    "financial_df = df_with_revenue.withColumn(\"margin_percentage\", (col(\"revenue_after_freight\") / col(\"price\")) * 100)\n",
    "\n",
    "print(\"--- Top 10 Orders by Profit Margin (Best) ---\")\n",
    "top_margins = financial_df.orderBy(col(\"margin_percentage\").desc())\n",
    "top_margins.select(\"order_id\", \"price\", \"freight_value\", \"margin_percentage\").show(10)\n",
    "\n",
    "print(\"\\n--- Bottom 10 Orders by Profit Margin (Worst) ---\")\n",
    "bottom_margins = financial_df.orderBy(col(\"margin_percentage\").asc())\n",
    "bottom_margins.select(\"order_id\", \"price\", \"freight_value\", \"margin_percentage\").show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T14:49:41.774261Z",
     "iopub.status.busy": "2025-09-02T14:49:41.773952Z",
     "iopub.status.idle": "2025-09-02T14:49:43.925213Z",
     "shell.execute_reply": "2025-09-02T14:49:43.924271Z",
     "shell.execute_reply.started": "2025-09-02T14:49:41.774242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "final_df = financial_df.join(products_df, on=\"product_id\", how=\"inner\")\n",
    "target_category = \"cama_mesa_banho\"\n",
    "category_df = final_df.filter(col(\"product_category_name\") == target_category)\n",
    "print(f\"--- Top 3 Highest Value Orders in Category '{target_category}' ---\")\n",
    "top_3_in_category = category_df.orderBy(col(\"price\").desc()).limit(3)\n",
    "top_3_in_category.select(\"order_id\", \"product_id\", \"product_category_name\", \"price\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T14:52:57.876932Z",
     "iopub.status.busy": "2025-09-02T14:52:57.876436Z",
     "iopub.status.idle": "2025-09-02T14:53:00.529069Z",
     "shell.execute_reply": "2025-09-02T14:53:00.527889Z",
     "shell.execute_reply.started": "2025-09-02T14:52:57.876897Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "top_cities_pandas = top_10_cities_by_revenue.toPandas()\n",
    "plt.figure(figsize=(12, 6)) \n",
    "sns.barplot(x=\"total_revenue\", y=\"customer_city\", data=top_cities_pandas, orient='h')\n",
    "plt.title('Top 10 Cities by Total Revenue', fontsize=16)\n",
    "plt.xlabel('Total Revenue', fontsize=12)\n",
    "plt.ylabel('City', fontsize=12)\n",
    "plt.show() \n",
    "revenue_by_month_pandas = revenue_by_month.orderBy(\"year\", \"month\").toPandas()\n",
    "# Create a proper date column for plotting\n",
    "revenue_by_month_pandas['date'] = pd.to_datetime(revenue_by_month_pandas['year'].astype(str) + '-' + revenue_by_month_pandas['month'].astype(str))\n",
    "plt.figure(figsize=(15, 7)) \n",
    "sns.lineplot(x='date', y='total_revenue', data=revenue_by_month_pandas, marker='o')\n",
    "plt.title('Total Monthly Revenue (2016-2018)', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Total Revenue', fontsize=12)\n",
    "plt.xticks(rotation=45) \n",
    "plt.grid(True) \n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T14:54:40.007848Z",
     "iopub.status.busy": "2025-09-02T14:54:40.007482Z",
     "iopub.status.idle": "2025-09-02T14:54:46.611643Z",
     "shell.execute_reply": "2025-09-02T14:54:46.610013Z",
     "shell.execute_reply.started": "2025-09-02T14:54:40.007825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, size, col\n",
    "print(\"Starting to group products by order...\")\n",
    "basket_data = final_df.groupBy(\"order_id\") \\\n",
    "                      .agg(collect_list(\"product_id\").alias(\"items\"))\n",
    "\n",
    "print(\"Grouping complete!\")\n",
    "print(\"\\n--- Sample of Basket Data (each row is one order) ---\")\n",
    "basket_data.select(\"items\").show(10, truncate=False)\n",
    "orders_with_multiple_items = basket_data.filter(size(col(\"items\")) > 1)\n",
    "print(f\"\\nAdditional Info: Number of orders with more than one item: {orders_with_multiple_items.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T14:59:02.759311Z",
     "iopub.status.busy": "2025-09-02T14:59:02.758957Z",
     "iopub.status.idle": "2025-09-02T14:59:09.780728Z",
     "shell.execute_reply": "2025-09-02T14:59:09.779875Z",
     "shell.execute_reply.started": "2025-09-02T14:59:02.759290Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.sql.functions import array_distinct\n",
    "\n",
    "cleaned_basket_data = basket_data.withColumn(\"items\", array_distinct(col(\"items\")))\n",
    "print(\"Training the FP-Growth model with relaxed settings...\")\n",
    "fpGrowth_relaxed = FPGrowth(itemsCol=\"items\", minSupport=0.0005, minConfidence=0.01)\n",
    "model_relaxed = fpGrowth_relaxed.fit(cleaned_basket_data)\n",
    "print(\"Model training complete!\")\n",
    "print(\"\\n--- Association Rules found by the relaxed model ---\")\n",
    "model_relaxed.associationRules.show(20, truncate=False)\n",
    "print(\"\\n--- Frequently purchased itemsets (relaxed model) ---\")\n",
    "model_relaxed.freqItemsets.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T15:04:51.021068Z",
     "iopub.status.busy": "2025-09-02T15:04:51.019852Z",
     "iopub.status.idle": "2025-09-02T15:04:56.012781Z",
     "shell.execute_reply": "2025-09-02T15:04:56.010007Z",
     "shell.execute_reply.started": "2025-09-02T15:04:51.021025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max, countDistinct, sum, datediff, lit\n",
    "from pyspark.sql import Window\n",
    "\n",
    "latest_date = full_df.select(max(\"order_purchase_timestamp\")).first()[0]\n",
    "current_date = latest_date.replace(day=latest_date.day + 1)\n",
    "print(f\"The 'current date' for Recency calculation is: {current_date}\")\n",
    "\n",
    "rfm_intermediate = full_df.groupBy(\"customer_id\").agg(\n",
    "    \n",
    "    sum(\"price\").alias(\"monetary\"),\n",
    "   \n",
    "    countDistinct(\"order_id\").alias(\"frequency\"),\n",
    "    \n",
    "   \n",
    "    max(\"order_purchase_timestamp\").alias(\"last_purchase_date\")\n",
    ")\n",
    "\n",
    "rfm_df = rfm_intermediate.withColumn(\n",
    "    \"recency\",\n",
    "    datediff(lit(current_date), col(\"last_purchase_date\"))\n",
    ")\n",
    "\n",
    "print(\"\\n--- RFM metrics for each customer ---\")\n",
    "rfm_df.select(\"customer_id\", \"recency\", \"frequency\", \"monetary\").show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T15:05:37.017592Z",
     "iopub.status.busy": "2025-09-02T15:05:37.017215Z",
     "iopub.status.idle": "2025-09-02T15:05:42.287969Z",
     "shell.execute_reply": "2025-09-02T15:05:42.282283Z",
     "shell.execute_reply.started": "2025-09-02T15:05:37.017569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ntile\n",
    "from pyspark.sql import Window\n",
    "recency_window = Window.orderBy(col(\"recency\").desc())\n",
    "\n",
    "frequency_window = Window.orderBy(col(\"frequency\").asc())\n",
    "monetary_window = Window.orderBy(col(\"monetary\").asc())\n",
    "rfm_scores = rfm_df.withColumn(\"r_score\", ntile(4).over(recency_window)) \\\n",
    "                   .withColumn(\"f_score\", ntile(4).over(frequency_window)) \\\n",
    "                   .withColumn(\"m_score\", ntile(4).over(monetary_window))\n",
    "\n",
    "print(\"--- RFM Scores for each customer ---\")\n",
    "rfm_scores.select(\"customer_id\", \"recency\", \"frequency\", \"monetary\", \"r_score\", \"f_score\", \"m_score\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T15:06:33.447941Z",
     "iopub.status.busy": "2025-09-02T15:06:33.447608Z",
     "iopub.status.idle": "2025-09-02T15:06:40.326172Z",
     "shell.execute_reply": "2025-09-02T15:06:40.323853Z",
     "shell.execute_reply.started": "2025-09-02T15:06:33.447922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, when\n",
    "\n",
    "rfm_segments = rfm_scores.withColumn(\n",
    "    \"rfm_segment\",\n",
    "    concat(col(\"r_score\"), col(\"f_score\"), col(\"m_score\"))\n",
    ")\n",
    "\n",
    "final_rfm = rfm_segments.withColumn(\n",
    "    \"segment_name\",\n",
    "    when(col(\"rfm_segment\").isin(\"444\", \"443\", \"434\", \"344\"), \"Champions\")\n",
    "    .when(col(\"rfm_segment\").isin(\"333\", \"334\", \"343\", \"433\"), \"Loyal Customers\")\n",
    "    .when(col(\"rfm_segment\").isin(\"441\", \"442\", \"431\", \"432\", \"422\", \"421\", \"341\", \"342\", \"331\", \"332\"), \"Potential Loyalists\")\n",
    "    .when(col(\"rfm_segment\").isin(\"411\", \"412\", \"423\"), \"New Customers\")\n",
    "    .when(col(\"rfm_segment\").isin(\"244\", \"243\", \"234\", \"144\", \"143\", \"134\"), \"At Risk\")\n",
    "    .when(col(\"rfm_segment\").isin(\"111\", \"112\", \"121\", \"122\", \"211\", \"212\", \"221\", \"222\"), \"Hibernating\")\n",
    "    .when(col(\"rfm_segment\").isin(\"133\", \"132\", \"142\", \"241\", \"231\", \"242\", \"232\"), \"About to Sleep\")\n",
    "    .otherwise(\"Others\") \n",
    ")\n",
    "\n",
    "print(\"--- Final Customer Segments ---\")\n",
    "final_rfm.select(\"customer_id\", \"recency\", \"frequency\", \"monetary\", \"rfm_segment\", \"segment_name\").show(20)\n",
    "print(\"\\n--- Customer Count by Segment ---\")\n",
    "final_rfm.groupBy(\"segment_name\").count().orderBy(col(\"count\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T15:09:09.953583Z",
     "iopub.status.busy": "2025-09-02T15:09:09.953120Z",
     "iopub.status.idle": "2025-09-02T15:09:25.914162Z",
     "shell.execute_reply": "2025-09-02T15:09:25.912995Z",
     "shell.execute_reply.started": "2025-09-02T15:09:09.953555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_path = \"/kaggle/working/\"\n",
    "\n",
    "print(f\"Starting to save final DataFrames to CSV files in: {output_path}\")\n",
    "final_rfm.repartition(1).write.csv(output_path + \"rfm_customer_segments\", header=True, mode=\"overwrite\")\n",
    "final_rfm.groupBy(\"segment_name\").count().orderBy(col(\"count\").desc()) \\\n",
    "    .repartition(1).write.csv(output_path + \"segment_counts\", header=True, mode=\"overwrite\")\n",
    "top_10_cities_by_revenue.repartition(1).write.csv(output_path + \"top_cities_revenue\", header=True, mode=\"overwrite\")\n",
    "revenue_by_month.repartition(1).write.csv(output_path + \"monthly_revenue\", header=True, mode=\"overwrite\")\n",
    "negative_margin_orders = financial_df.filter(col(\"margin_percentage\") < 0)\n",
    "negative_margin_orders.repartition(1).write.csv(output_path + \"negative_margin_orders\", header=True, mode=\"overwrite\")\n",
    "print(\"\\nAll files have been saved successfully!\")\n",
    "print(\"You can now find them in the 'Output' section of your Kaggle notebook.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 55151,
     "sourceId": 2669146,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
